import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from pylon.constraint import constraint
from pylon.tnorm_solver import *
from pylon.sampling_solver import WeightedSamplingSolver
from pylon.circuit_solver import SemanticLossCircuitSolver


def get_solvers(num_samples):
    return [WeightedSamplingSolver(num_samples)]


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 20)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)

        return x.reshape(-1, 10, 2)


def train(constraint=None, epoch=100):

    net = Net()
    X, y = get_mnist_data()
    X = torch.cat((X, X, X))
    y = torch.cat((y, y, y))
    optimizer = optim.Adadelta(net.parameters(), lr=1.0)

    for i in range(epoch):

        optimizer.zero_grad()

        output = net(X)
        loss = F.cross_entropy(output[:, :, 1], y)

        if constraint:
            loss += constraint(output)

        loss.backward()
        optimizer.step()

    return net


def only_one(x):
    return torch.sum(x, 1) == 1


def test_only_one_mnist():

    X, y = get_mnist_data()
    X = torch.cat((X, X, X))
    y = torch.cat((y, y, y))

    for solver in get_solvers(num_samples=200):
        only_one_constraint = constraint(only_one, solver)
        net = train(only_one_constraint)

        assert(torch.argmax(torch.softmax(net(X), dim=-1), dim=-1).sum().item() == 3)


def get_mnist_data():
    X = torch.tensor([[[
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242,  0.2249,  1.5996,  2.7960,  1.5996,  0.2122, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         0.1867,  2.6051,  2.7833,  2.7833,  2.7833,  2.5924, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.2631,
         2.4651,  2.7960,  2.7833,  2.6178,  2.5415,  2.7833,  0.3013,
         -0.3478, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.2969,  0.3395,  2.4269,
         2.7833,  2.7960,  2.7833,  2.1469,  0.6450,  2.7833,  2.7960,
         1.1286, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242,  1.6505,  2.7833,  2.7833,
         2.7833,  2.7960,  2.7833,  2.7833,  0.7977,  1.9814,  2.7960,
         1.7014, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242,  0.2249,  2.6051,  2.7960,  2.7960,
         1.9942,  1.0268,  2.7960,  2.4778,  0.1740,  0.5813,  2.8215,
         1.7141, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242,  0.1867,  2.6051,  2.7833,  2.7833,  1.8541,
         -0.2715,  0.5304,  1.1159, -0.1569, -0.4242, -0.4242,  2.7960,
         2.6687,  0.2122, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242,  0.0595,  1.6759,  2.7960,  2.5415,  2.2233,  0.6450,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.7960,
         2.7833,  1.6759, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.3351,  1.8414,  2.7833,  2.6306,  0.4795, -0.1824, -0.0678,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.7960,
         2.7833,  2.0578, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         0.3013,  2.7833,  2.7833,  0.3777, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.7960,
         2.7833,  2.0578, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         2.0960,  2.7960,  1.9942, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.8215,
         2.7960,  2.0705, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.5431,
         2.7069,  2.7833,  1.0013, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.7960,
         2.7833,  1.4596, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,
         2.7833,  2.5033, -0.1060, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.3351,  1.2941,  2.7960,
         1.9432, -0.2715, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,
         2.7833,  2.4142, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.3351,  1.2432,  2.7833,  2.4396,
         0.4795, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,
         2.7833,  1.4214, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242,  0.1867,  1.6759,  2.7833,  1.7778, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6704,
         2.7960,  2.4396, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242,  1.0268,  2.6051,  2.7960,  1.6378, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,
         2.7833,  2.7451,  1.4341,  0.1867, -0.0551,  0.6577,  1.8414,
         2.4396,  2.7960,  2.4142,  1.7014,  0.2886, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,
         2.7833,  2.7833,  2.7833,  2.4906,  2.3124,  2.7833,  2.7833,
         2.7833,  2.0705,  1.2305, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.0678,
         2.1087,  2.7833,  2.7833,  2.7960,  2.7833,  2.7833,  2.5415,
         1.4214, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.1060,  1.2050,  2.7833,  2.7960,  2.7833,  1.3705,  0.0467,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],
        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,
         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]])
    y = torch.tensor([0])

    return X, y
