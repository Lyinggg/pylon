{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI Transitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task measures three connected NLI examples according to their transitive consistency.\n",
    "For instance, suppose we have three sentences P, H, and Z.\n",
    "Given a NLI model, if model predicts entailment (E) to the example (P, H) and example (H, Z),\n",
    "then we may claim the model should also predicts entailment to the example (P, Z) according to transitivity.\n",
    "\n",
    "To demonstrate how our Pylon works on this task, we will compare\n",
    "1. train a baseline model on a labeled set of NLI examples and evaluate on our transitivity data;\n",
    "2. train a constrained model (using transitivity rule) and evaluate on the transitivity data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import random\n",
    "from transformers import *\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define how to process the data, batch them up, and ready them for training.\n",
    "Suppose we are going to use DistilBERT as our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data...\n",
      "loading from ./nli//snli.train.p.txt\n",
      "loading from ./nli//snli.train.h.txt\n",
      "loading from ./nli//snli.train.label.txt\n",
      "loading from ./nli//mscoco.train.p.txt\n",
      "loading from ./nli//mscoco.train.h.txt\n",
      "loading from ./nli//mscoco.train.z.txt\n",
      "loading from ./nli//mscoco.test.p.txt\n",
      "loading from ./nli//mscoco.test.h.txt\n",
      "loading from ./nli//mscoco.test.z.txt\n",
      "loading from ./nli//snli.val.p.txt\n",
      "loading from ./nli//snli.val.h.txt\n",
      "loading from ./nli//snli.val.label.txt\n"
     ]
    }
   ],
   "source": [
    "LABEL_TO_ID = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "ENT = LABEL_TO_ID['entailment']\n",
    "CON = LABEL_TO_ID['contradiction']\n",
    "NEU = LABEL_TO_ID['neutral']\n",
    "\n",
    "config = AutoConfig.from_pretrained('distilbert-base-uncased')\n",
    "config.num_labels = len(LABEL_TO_ID)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def process_data(tokenizer, path):\n",
    "    def batch_encode(all_p, all_h):\n",
    "        return tokenizer.batch_encode_plus([(p, h) for p, h in zip(all_p, all_h)], max_length=100, padding=True, return_tensors='pt')\n",
    "\n",
    "    files = ['snli.train.p.txt', 'snli.train.h.txt', 'snli.train.label.txt', \\\n",
    "        'mscoco.train.p.txt', 'mscoco.train.h.txt', 'mscoco.train.z.txt', \\\n",
    "        'mscoco.test.p.txt', 'mscoco.test.h.txt', 'mscoco.test.z.txt', \\\n",
    "        'snli.val.p.txt', 'snli.val.h.txt', 'snli.val.label.txt']\n",
    "    all_data = []\n",
    "    for file in files:\n",
    "        all_data.append([])\n",
    "        file = path + '/' + file\n",
    "        with open(file, 'r') as f:\n",
    "            print('loading from', file)\n",
    "            for line in f:\n",
    "                if line.strip() == '':\n",
    "                    continue\n",
    "                all_data[-1].append(line.strip())\n",
    "\n",
    "    snli_train = batch_encode(all_data[0], all_data[1])['input_ids']\n",
    "    snli_train_label = torch.tensor([LABEL_TO_ID[l] for l in all_data[2]], dtype=torch.long)\n",
    "    mscoco_train_ph = batch_encode(all_data[3], all_data[4])['input_ids']\n",
    "    mscoco_train_hz = batch_encode(all_data[4], all_data[5])['input_ids']\n",
    "    mscoco_train_pz = batch_encode(all_data[3], all_data[5])['input_ids']\n",
    "    mscoco_test_ph = batch_encode(all_data[6], all_data[7])['input_ids']\n",
    "    mscoco_test_hz = batch_encode(all_data[7], all_data[8])['input_ids']\n",
    "    mscoco_test_pz = batch_encode(all_data[6], all_data[8])['input_ids']\n",
    "    snli_test = batch_encode(all_data[9], all_data[10])['input_ids']\n",
    "    snli_test_label = torch.tensor([LABEL_TO_ID[l] for l in all_data[11]], dtype=torch.long)\n",
    "\n",
    "    snli_train = TensorDataset(snli_train, snli_train_label)\n",
    "    mscoco_train = TensorDataset(mscoco_train_ph, mscoco_train_hz, mscoco_train_pz)\n",
    "    mscoco_test = TensorDataset(mscoco_test_ph, mscoco_test_hz, mscoco_test_pz)\n",
    "    snli_test = TensorDataset(snli_test, snli_test_label)\n",
    "    return snli_train, mscoco_train, mscoco_test, snli_test\n",
    "\n",
    "# preprocess and batch up data\n",
    "print('processing data...')\n",
    "snli_train, mscoco_train, mscoco_test, snli_test = process_data(tokenizer, './nli/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define metrics for transtivity violation. It prints model accuracy on labeled test sets and two metrics for violation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, trans_data, test_data, batch_size=8, device=torch.device('cpu')):\n",
    "    trans_data_loader = DataLoader(trans_data, sampler=SequentialSampler(trans_data), batch_size=batch_size)\n",
    "    ex_cnt = 0\n",
    "    global_sat = 0.0\n",
    "    conditional_sat = []\n",
    "    model = model.to(device)\n",
    "    for _, batch in enumerate(trans_data_loader):\n",
    "        with torch.no_grad():\n",
    "            logits_ph = model(input_ids=batch[0].to(device), return_dict=True).logits\n",
    "            logits_hz = model(input_ids=batch[1].to(device), return_dict=True).logits\n",
    "            logits_pz = model(input_ids=batch[2].to(device), return_dict=True).logits\n",
    "    \n",
    "            ph_y = torch.softmax(logits_ph.view(-1, len(LABEL_TO_ID)), dim=-1)\n",
    "            hz_y = torch.softmax(logits_hz.view(-1, len(LABEL_TO_ID)), dim=-1)\n",
    "            pz_y = torch.softmax(logits_pz.view(-1, len(LABEL_TO_ID)), dim=-1)\n",
    "    \n",
    "            ph_y_mask = (ph_y == ph_y.max(-1)[0].unsqueeze(-1))\n",
    "            hz_y_mask = (hz_y == hz_y.max(-1)[0].unsqueeze(-1))\n",
    "            pz_y_mask = (pz_y == pz_y.max(-1)[0].unsqueeze(-1))\n",
    "    \n",
    "            lhs, satisfied = transitivity_check(ph_y_mask, hz_y_mask, pz_y_mask)\n",
    "            global_sat += float(satisfied.sum())\n",
    "            conditional_sat.extend([float(s) for l, s in zip(lhs, satisfied) if l])\n",
    "            ex_cnt += batch[0].shape[0]\n",
    "        \n",
    "    print('Global percent of predictions that violate the transitivity constraint', \n",
    "        1-global_sat/ex_cnt)\n",
    "    conditional_sat = sum(conditional_sat)/len(conditional_sat) if len(conditional_sat) != 0 else 1\n",
    "    print('Conditional percent of predictions that violate the transitivity constraint', \n",
    "        1-conditional_sat)\n",
    "\n",
    "    test_data_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)\n",
    "    ex_cnt = 0\n",
    "    correct_cnt = 0\n",
    "    for _, batch in enumerate(test_data_loader):\n",
    "        with torch.no_grad():\n",
    "            logits_ph = model(input_ids=batch[0].to(device), return_dict=True).logits\n",
    "            pred_ph = logits_ph.cpu().argmax(-1)\n",
    "            gold_ph = batch[1]\n",
    "            correct_cnt += int((pred_ph == gold_ph).sum().item())\n",
    "            ex_cnt += batch[0].shape[0]\n",
    "    print('test set accuracy', correct_cnt/ex_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we can optionally specify to use a constraint function which will produce a constraint loss.\n",
    "If use_trans is True, the constraint loss will participate in training; otherwise the constraint has 0 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: transformer for sequence classification\n",
    "def train(model, constraint_func, train_gold, train_trans, \n",
    "            lr=5e-5, batch_size=8, seed=1, grad_clip=1.0, lambda_trans=1, epoch=1,\n",
    "            use_gold=True, use_trans=True,\n",
    "            device=torch.device('cpu')):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    train_gold_loader = DataLoader(train_gold, sampler=RandomSampler(train_gold), batch_size=batch_size)\n",
    "    train_trans_loader = DataLoader(train_trans, sampler=RandomSampler(train_trans), batch_size=batch_size)\n",
    "\n",
    "    # mixing two datasets\n",
    "    data_loaders = [train_gold_loader, train_trans_loader]\n",
    "    expanded_data_loader = [train_gold_loader] * len(train_gold_loader) + [train_trans_loader] * len(train_trans_loader)\n",
    "    random.shuffle(expanded_data_loader)\n",
    "\n",
    "    # create optimizer\n",
    "    weight_decay = 0\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    named_params = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "    total_updates = epoch * len(expanded_data_loader)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_updates)\n",
    "\n",
    "    update_cnt = 0\n",
    "    loss_accumulator = 0.0\n",
    "    model = model.to(device)\n",
    "    model.zero_grad()\n",
    "    for epoch_id in range(epoch):\n",
    "        iters = [loader.__iter__() for loader in data_loaders]\n",
    "        for loader in expanded_data_loader:\n",
    "            batch = next(iters[data_loaders.index(loader)])\n",
    "\n",
    "            # if current batch is labeled data\n",
    "            if loader is train_gold_loader:\n",
    "                if not use_gold:\n",
    "                    continue\n",
    "                output = model(input_ids=batch[0].to(device), labels=batch[1].to(device), return_dict=True)\n",
    "                loss = output[0]\n",
    "\n",
    "            elif loader is train_trans_loader:\n",
    "                if not use_trans:\n",
    "                    continue\n",
    "                logits_ph = model(input_ids=batch[0].to(device), return_dict=True).logits\n",
    "                logits_hz = model(input_ids=batch[1].to(device), return_dict=True).logits\n",
    "                logits_pz = model(input_ids=batch[2].to(device), return_dict=True).logits\n",
    "\n",
    "                constrain_func = constraint(constraint_func)\n",
    "                loss = constrain_func(logits_ph, logits_hz, logits_pz)\n",
    "                loss = loss * lambda_trans\n",
    "\n",
    "            else:\n",
    "                raise Exception('unrecognized loader')\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss_accumulator += (loss.item())\n",
    "            update_cnt += 1\n",
    "\n",
    "            if update_cnt % 100 == 0:\n",
    "                print('trained {0} steps, avg loss {1:4f}'.format(update_cnt, float(loss_accumulator/update_cnt)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a constraint function for transitivity. The constraint below is a generalization of the example we showed at the beginning.\n",
    "We will use the constraint loss along with the standard cross entropy loss to train our constrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing models and solvers...\n"
     ]
    }
   ],
   "source": [
    "from pytorch_constraints.constraint import constraint\n",
    "\n",
    "# the actual constraint function used for t-norm solvers\n",
    "# inputs are logits predictions\n",
    "def transitivity(ph_batch, hz_batch, pz_batch):\n",
    "    ee_e = (ph_batch[:, ENT]).logical_and(hz_batch[:, ENT]) <= (pz_batch[:, ENT])\n",
    "    ec_c = (ph_batch[:, ENT]).logical_and(hz_batch[:, CON]) <= (pz_batch[:, CON])\n",
    "    ne_notc = (ph_batch[:, NEU]).logical_and(hz_batch[:, ENT]) <= (pz_batch[:, CON]).logical_not()\n",
    "    nc_note = (ph_batch[:, CON]).logical_and(hz_batch[:, NEU]) <= (pz_batch[:, CON]).logical_not()\n",
    "    return ee_e.logical_and(ec_c).logical_and(ne_notc).logical_and(nc_note)\n",
    "\n",
    "# checking if constraint is satisfied\n",
    "# inputs are binary tensors\n",
    "def transitivity_check(ph_y_mask, hz_y_mask, pz_y_mask):\n",
    "    ee = ph_y_mask[:, ENT].logical_and(hz_y_mask[:, ENT])\n",
    "    ec = ph_y_mask[:, ENT].logical_and(hz_y_mask[:, CON])\n",
    "    ne = ph_y_mask[:, NEU].logical_and(hz_y_mask[:, ENT])\n",
    "    nc = ph_y_mask[:, NEU].logical_and(hz_y_mask[:, CON])\n",
    "\n",
    "    ee_e = ee.logical_not().logical_or(pz_y_mask[:, ENT])\n",
    "    ec_c = ec.logical_not().logical_or(pz_y_mask[:, CON])\n",
    "    ne_notc = ne.logical_not().logical_or(pz_y_mask[:, CON].logical_not())\n",
    "    nc_note = nc.logical_not().logical_or(pz_y_mask[:, ENT].logical_not())\n",
    "\n",
    "    lhs = ee.logical_or(ec).logical_or(ne).logical_or(nc)\n",
    "    return lhs, ee_e.logical_and(ec_c).logical_and(ne_notc).logical_and(nc_note)\n",
    "\n",
    "print('initializing models and solvers...')\n",
    "constraint_func = transitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train a distilbert NLI model and evaluate it on the transitivity test set\n",
    "The pipeline is the following:\n",
    "1. first train a model purely on labeled SNLI data, this will give us a model with reasonably good accuracy\n",
    "2. continue training this model on the union of labeled SNLI data and transitivity training split, the training loss will be cross entropy and constraint loss\n",
    "\n",
    "The expectation is to see whether adding constraint will help reducint transitivity violations, i.e., comparing step 1 and step 2, we should see violation drop without trading off accuracy\n",
    "We will also train a baseline model (w/o constraint and transitivity data) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline model...\n",
      "step 1\n",
      "trained 100 steps, avg loss 1.114885\n",
      "trained 200 steps, avg loss 1.111184\n",
      "trained 300 steps, avg loss 1.087879\n",
      "trained 400 steps, avg loss 1.029931\n",
      "trained 500 steps, avg loss 0.983806\n",
      "trained 600 steps, avg loss 0.943567\n",
      "Global percent of predictions that violate the transitivity constraint 0.11719999999999997\n",
      "Conditional percent of predictions that violate the transitivity constraint 0.223578786722625\n",
      "test set accuracy 0.7372\n",
      "step 2\n",
      "trained 100 steps, avg loss 0.656024\n",
      "trained 200 steps, avg loss 0.649070\n",
      "trained 300 steps, avg loss 0.616851\n",
      "trained 400 steps, avg loss 0.575583\n",
      "trained 500 steps, avg loss 0.559830\n",
      "trained 600 steps, avg loss 0.530038\n",
      "Global percent of predictions that violate the transitivity constraint 0.08819999999999995\n",
      "Conditional percent of predictions that violate the transitivity constraint 0.19427312775330396\n",
      "test set accuracy 0.7634\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "#device = torch.device(\"cuda\", 0)\n",
    "\n",
    "print('training baseline model...')\n",
    "print('step 1')\n",
    "model = train(model, constraint_func, snli_train, mscoco_train, lr=5e-5, epoch=1, use_gold=True, use_trans=False, device=device)\n",
    "evaluate(model, mscoco_test, snli_test, device=device)\n",
    "print('step 2')\n",
    "# note that we set use_trans=False, so the constraint will not be used\n",
    "model = train(model, constraint_func, snli_train, mscoco_train, lr=2e-5, epoch=1, use_gold=True, use_trans=False, device=device)\n",
    "evaluate(model, mscoco_test, snli_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our constrained model.\n",
    "We will see that it maintains similar test set accuracy while substantially reduce violation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model with product t-norm solver...\n",
      "step 1\n",
      "trained 100 steps, avg loss 0.497041\n",
      "trained 200 steps, avg loss 0.506893\n",
      "trained 300 steps, avg loss 0.492820\n",
      "trained 400 steps, avg loss 0.468921\n",
      "trained 500 steps, avg loss 0.468942\n",
      "trained 600 steps, avg loss 0.452199\n",
      "Global percent of predictions that violate the transitivity constraint 0.10899999999999999\n",
      "Conditional percent of predictions that violate the transitivity constraint 0.19854280510018218\n",
      "test set accuracy 0.7522\n",
      "step 2\n",
      "trained 100 steps, avg loss 0.366783\n",
      "trained 200 steps, avg loss 0.356319\n",
      "trained 300 steps, avg loss 0.315683\n",
      "trained 400 steps, avg loss 0.301935\n",
      "trained 500 steps, avg loss 0.283556\n",
      "trained 600 steps, avg loss 0.289011\n",
      "trained 700 steps, avg loss 0.278423\n",
      "trained 800 steps, avg loss 0.282109\n",
      "trained 900 steps, avg loss 0.278563\n",
      "trained 1000 steps, avg loss 0.270365\n",
      "trained 1100 steps, avg loss 0.265088\n",
      "trained 1200 steps, avg loss 0.268299\n",
      "trained 1300 steps, avg loss 0.264701\n",
      "Global percent of predictions that violate the transitivity constraint 0.020399999999999974\n",
      "Conditional percent of predictions that violate the transitivity constraint 0.08557046979865768\n",
      "test set accuracy 0.7614\n"
     ]
    }
   ],
   "source": [
    "print('training model with product t-norm solver...')\n",
    "print('step 1')\n",
    "model = train(model, constraint_func, snli_train, mscoco_train, lr=5e-5, epoch=1, use_gold=True, use_trans=False, device=device)\n",
    "evaluate(model, mscoco_test, snli_test, device=device)\n",
    "print('step 2')\n",
    "# Now we set use_trans=True, so the constraint will be used\n",
    "model = train(model, constraint_func, snli_train, mscoco_train, lr=2e-5, epoch=1, use_gold=True, use_trans=True, device=device)\n",
    "evaluate(model, mscoco_test, snli_test, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
